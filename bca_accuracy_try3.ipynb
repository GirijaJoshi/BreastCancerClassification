{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Girija Joshi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Girija Joshi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Girija Joshi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Girija Joshi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Girija Joshi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Girija Joshi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Girija Joshi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Girija Joshi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Girija Joshi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Girija Joshi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Girija Joshi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Girija Joshi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait getting all images.....\n"
     ]
    }
   ],
   "source": [
    "# getting names of all the files in IDC_regular_ps50_idx5and sub dirs\n",
    "print(\"Please wait getting all images.....\")\n",
    "# files = glob('Resources/IDC_regular_ps50_idx5/*/*/*')\n",
    "files = glob('C:\\BootCamp\\BreastCancerClassification\\equal_images/*/*/*')\n",
    "\n",
    "# Example : Resources/IDC_regular_ps50_idx5\\\\10254\\\\0\\\\10254_idx5_x1001_y1001_class0.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking how many are cancer files which has class1 in it\n",
    "count =0 \n",
    "for file in files:\n",
    "    if 'class1' in file:\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "Number Of 1: 68378\n",
      "Number Of 0: 68378\n",
      "Total: 136756\n"
     ]
    }
   ],
   "source": [
    "print('------------------')\n",
    "\n",
    "print(f'Number Of 1: {count}')\n",
    "print(f'Number Of 0: {len(files) - count}')\n",
    "# total number of files\n",
    "print(f'Total: {len(files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 80% of total for training and 20% for testing\n",
    "train_num = int(len(files) * 0.80)\n",
    "test_num = len(files) - train_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and y train data\n",
    "def find_data(files, lower_limit, upper_limit):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # tqdm(patient_ids)\n",
    "    for file in tqdm(files[lower_limit:upper_limit]):\n",
    "        if file.endswith(\".png\"):\n",
    "            # Convering cureent image into PIL image format. PIL image format is RGB format.\n",
    "            img = tf.keras.preprocessing.image.load_img(file, target_size = (50,50))\n",
    "\n",
    "            # Keras provides the img_to_array() function for converting a loaded image in PIL format into a NumPy array \n",
    "            #for use with deep learning models. The image is convertated into t [height, width, channels]\n",
    "\n",
    "            # # Arguments\n",
    "            #         img: PIL Image instance.\n",
    "            #         data_format: Image data format,\n",
    "            #             either \"channels_first\" or \"channels_last\".\n",
    "            #         dtype: Dtype to use for the returned array.\n",
    "            # Returns: A 3D Numpy array.\n",
    "            \n",
    "            # The component values are often stored as integer numbers in the range 0 to 255, \n",
    "            # the range that a single 8-bit byte can offer,\n",
    "            pixels = tf.keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "            # converting into 0 - 1, This is called as Normalization will help you to remove distortions \n",
    "            # caused by lights and shadows in an image.\n",
    "            pixels /= 255\n",
    "            X.append(pixels)\n",
    "            if 'class1' in file:\n",
    "                y.append(1)\n",
    "            elif 'class0' in file:\n",
    "                y.append(0)\n",
    "    return np.stack(X), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of training files: 109404\n",
      "Num of test files:27352\n"
     ]
    }
   ],
   "source": [
    "print(f'Num of training files: {train_num}\\nNum of test files:{test_num}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 109404/109404 [19:12<00:00, 94.90it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train = find_data(files,0, train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 27352/27352 [04:58<00:00, 91.64it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = find_data(files, train_num, len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='count'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAD4CAYAAADGmmByAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQkElEQVR4nO3df6xf9V3H8eeLdmOoK/KjYNcyS0JjLOi2UCtx/jFXI/XXIAssnZk0rkkNQbMlRgX/cFPTZMQpjjlIGpkU1EHDROoSVFLExUhgF51CYYSbMaFppR1FxkzAFN/+8f3c7dvbby+3/fR7by/3+UhOvue8z/mc+zk3N3nlcz7ne26qCkmSTtRp890BSdLCZpBIkroYJJKkLgaJJKmLQSJJ6rJ0vjsw184999xavXr1fHdDkhaUxx577JtVtXzUvkUXJKtXr2ZiYmK+uyFJC0qS/zzWPm9tSZK6GCSSpC4GiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkrosum+2nwyX/uYd890FnYIe+8Nr5rsL0rxwRCJJ6uKIRHoTee73f2S+u6BT0Dt/9/Gxnt8RiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC5jDZIk30jyeJKvJplotbOTPJDkmfZ51tDxNySZTPJ0ksuH6pe280wmuTlJWv30JHe3+iNJVo/zeiRJR5uLEclPVdW7q2pd274e2F1Va4DdbZska4FNwMXARuCWJEtam1uBrcCatmxs9S3AS1V1EXATcOMcXI8kach83Nq6AtjR1ncAVw7V76qq16rqWWASWJ9kBbCsqh6uqgLumNZm6lz3ABumRiuSpLkx7iAp4B+SPJZka6udX1X7Adrnea2+Enh+qO3eVlvZ1qfXj2hTVYeBl4FzxnAdkqRjGPc/tnpvVe1Lch7wQJKvzXDsqJFEzVCfqc2RJx6E2FaAd77znTP3WJJ0XMY6Iqmqfe3zAHAvsB54od2uon0eaIfvBS4Yar4K2Nfqq0bUj2iTZClwJnBoRD+2V9W6qlq3fPnyk3NxkiRgjEGS5HuTvH1qHfgZ4AlgF7C5HbYZuK+t7wI2tSexLmQwqf5ou/31SpLL2vzHNdPaTJ3rKuDBNo8iSZoj47y1dT5wb5v7Xgr8VVX9XZKvADuTbAGeA64GqKo9SXYCTwKHgeuq6vV2rmuB24EzgPvbAnAbcGeSSQYjkU1jvB5J0ghjC5Kq+jrwrhH1F4ENx2izDdg2oj4BXDKi/iotiCRJ88NvtkuSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4GiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4GiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkrqMPUiSLEnyb0m+1LbPTvJAkmfa51lDx96QZDLJ00kuH6pfmuTxtu/mJGn105Pc3eqPJFk97uuRJB1pLkYkHwOeGtq+HthdVWuA3W2bJGuBTcDFwEbgliRLWptbga3AmrZsbPUtwEtVdRFwE3DjeC9FkjTdWIMkySrg54E/GypfAexo6zuAK4fqd1XVa1X1LDAJrE+yAlhWVQ9XVQF3TGszda57gA1ToxVJ0twY94jkT4DfAv5vqHZ+Ve0HaJ/ntfpK4Pmh4/a22sq2Pr1+RJuqOgy8DJwzvRNJtiaZSDJx8ODBzkuSJA0bW5Ak+QXgQFU9NtsmI2o1Q32mNkcWqrZX1bqqWrd8+fJZdkeSNBtLx3ju9wIfSPJzwNuAZUn+AnghyYqq2t9uWx1ox+8FLhhqvwrY1+qrRtSH2+xNshQ4Ezg0rguSJB1tbCOSqrqhqlZV1WoGk+gPVtVHgF3A5nbYZuC+tr4L2NSexLqQwaT6o+321ytJLmvzH9dMazN1rqvazzhqRCJJGp9xjkiO5VPAziRbgOeAqwGqak+SncCTwGHguqp6vbW5FrgdOAO4vy0AtwF3JplkMBLZNFcXIUkamJMgqaqHgIfa+ovAhmMctw3YNqI+AVwyov4qLYgkSfPDb7ZLkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4GiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4GiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkrrMKkiS7J5NTZK0+CydaWeStwHfA5yb5Cwgbdcy4B1j7pskaQGYMUiAXwU+ziA0HuO7QfIt4HPj65YkaaGYMUiq6jPAZ5L8elV9do76JElaQGY1R1JVn03yE0l+Kck1U8tMbZK8LcmjSf49yZ4kv9fqZyd5IMkz7fOsoTY3JJlM8nSSy4fqlyZ5vO27OUla/fQkd7f6I0lWn9BvQZJ0wmY72X4n8GngJ4Efa8u6N2j2GvD+qnoX8G5gY5LLgOuB3VW1BtjdtkmyFtgEXAxsBG5JsqSd61ZgK7CmLRtbfQvwUlVdBNwE3Dib65EknTxvNEcyZR2wtqpqtidux367bb6lLQVcAbyv1XcADwG/3ep3VdVrwLNJJoH1Sb4BLKuqhwGS3AFcCdzf2nyynese4E+T5Hj6KUnqM9vvkTwB/MDxnjzJkiRfBQ4AD1TVI8D5VbUfoH2e1w5fCTw/1Hxvq61s69PrR7SpqsPAy8A5I/qxNclEkomDBw8e72VIkmYw2xHJucCTSR5lcMsKgKr6wEyNqup14N1Jvh+4N8klMxyeEbWaoT5Tm+n92A5sB1i3bp2jFUk6iWYbJJ/s+SFV9d9JHmIwt/FCkhVVtT/JCgajFRiMNC4YarYK2Nfqq0bUh9vsTbIUOBM41NNXSdLxme1TW/80apmpTZLlbSRCkjOAnwa+BuwCNrfDNgP3tfVdwKb2JNaFDCbVH223v15Jcll7WuuaaW2mznUV8KDzI5I0t2Y1IknyCt+9ZfRWBhPn/1NVy2ZotgLY0Z68Og3YWVVfSvIwsDPJFuA54GqAqtqTZCfwJHAYuK7dGgO4FrgdOIPBJPv9rX4bcGebmD/E4KkvSdIcmlWQVNXbh7eTXAmsf4M2/wG8Z0T9RWDDMdpsA7aNqE8AR82vVNWrtCCSJM2PE3r7b1X9DfD+k9sVSdJCNNtbWx8c2jyNwfdKnIuQJM36qa1fHFo/DHyDwZcBJUmL3GznSH5l3B2RJC1Ms33X1qok9yY5kOSFJF9MsuqNW0qS3uxmO9n+5wy+s/EOBq8l+dtWkyQtcrMNkuVV9edVdbgttwPLx9gvSdICMdsg+WaSj7SXMC5J8hHgxXF2TJK0MMw2SD4KfAj4L2A/g9eROAEvSZr1479/AGyuqpdg8F8OGfyjq4+Oq2OSpIVhtiOSH50KEYCqOsSI159Ikhaf2QbJadP+t/rZzH40I0l6E5ttGPwR8C9J7mHwapQPMeLlipKkxWe232y/I8kEgxc1BvhgVT051p5JkhaEWd+easFheEiSjnBCr5GXJGmKQSJJ6mKQSJK6GCSSpC4GiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKnL2IIkyQVJ/jHJU0n2JPlYq5+d5IEkz7TP4X+YdUOSySRPJ7l8qH5pksfbvpuTpNVPT3J3qz+SZPW4rkeSNNo4RySHgd+oqh8GLgOuS7IWuB7YXVVrgN1tm7ZvE3AxsBG4JcmSdq5bga3AmrZsbPUtwEtVdRFwE3DjGK9HkjTC2IKkqvZX1b+29VeAp4CVwBXAjnbYDuDKtn4FcFdVvVZVzwKTwPokK4BlVfVwVRVwx7Q2U+e6B9gwNVqRJM2NOZkjabec3gM8ApxfVfthEDbAee2wlcDzQ832ttrKtj69fkSbqjoMvAycM5aLkCSNNPYgSfJ9wBeBj1fVt2Y6dEStZqjP1GZ6H7YmmUgycfDgwTfqsiTpOIw1SJK8hUGI/GVV/XUrv9BuV9E+D7T6XuCCoeargH2tvmpE/Yg2SZYCZwKHpvejqrZX1bqqWrd8+fKTcWmSpGacT20FuA14qqr+eGjXLmBzW98M3DdU39SexLqQwaT6o+321ytJLmvnvGZam6lzXQU82OZRJElzZOkYz/1e4JeBx5N8tdV+B/gUsDPJFuA54GqAqtqTZCfwJIMnvq6rqtdbu2uB24EzgPvbAoOgujPJJIORyKYxXo8kaYSxBUlV/TOj5zAANhyjzTZg24j6BHDJiPqrtCCSJM0Pv9kuSepikEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4GiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4GiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqMrYgSfL5JAeSPDFUOzvJA0meaZ9nDe27IclkkqeTXD5UvzTJ423fzUnS6qcnubvVH0myelzXIkk6tnGOSG4HNk6rXQ/srqo1wO62TZK1wCbg4tbmliRLWptbga3AmrZMnXML8FJVXQTcBNw4tiuRJB3T2IKkqr4MHJpWvgLY0dZ3AFcO1e+qqteq6llgElifZAWwrKoerqoC7pjWZupc9wAbpkYrkqS5M9dzJOdX1X6A9nleq68Enh86bm+rrWzr0+tHtKmqw8DLwDmjfmiSrUkmkkwcPHjwJF2KJAlOncn2USOJmqE+U5uji1Xbq2pdVa1bvnz5CXZRkjTKXAfJC+12Fe3zQKvvBS4YOm4VsK/VV42oH9EmyVLgTI6+lSZJGrO5DpJdwOa2vhm4b6i+qT2JdSGDSfVH2+2vV5Jc1uY/rpnWZupcVwEPtnkUSdIcWjquEyf5AvA+4Nwke4FPAJ8CdibZAjwHXA1QVXuS7ASeBA4D11XV6+1U1zJ4AuwM4P62ANwG3JlkksFIZNO4rkWSdGxjC5Kq+vAxdm04xvHbgG0j6hPAJSPqr9KCSJI0f06VyXZJ0gJlkEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4GiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4GiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqsuCDJMnGJE8nmUxy/Xz3R5IWmwUdJEmWAJ8DfhZYC3w4ydr57ZUkLS4LOkiA9cBkVX29qv4XuAu4Yp77JEmLytL57kCnlcDzQ9t7gR+fflCSrcDWtvntJE/PQd8Wi3OBb853J04F+fTm+e6CjuTf5pRP5GSc5QePtWOhB8mo304dVajaDmwff3cWnyQTVbVuvvshTeff5txZ6Le29gIXDG2vAvbNU18kaVFa6EHyFWBNkguTvBXYBOya5z5J0qKyoG9tVdXhJL8G/D2wBPh8Ve2Z524tNt4y1KnKv805kqqjphQkSZq1hX5rS5I0zwwSSVIXg0QnxFfT6FSV5PNJDiR5Yr77slgYJDpuvppGp7jbgY3z3YnFxCDRifDVNDplVdWXgUPz3Y/FxCDRiRj1apqV89QXSfPMINGJmNWraSQtDgaJToSvppH0HQaJToSvppH0HQaJjltVHQamXk3zFLDTV9PoVJHkC8DDwA8l2Ztky3z36c3OV6RIkro4IpEkdTFIJEldDBJJUheDRJLUxSCRJHUxSCRJXQwSSVKX/wec7W0YHoXvBwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='count'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Conv2D, Activation, MaxPooling2D, Flatten, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109404, 50, 50, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train2 is 4 diemtion we need to convert in 2D\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_model(inp_shape = (50,50,3)):\n",
    "    inp = Input(inp_shape)\n",
    "    m = Conv2D(32, (3,3), kernel_initializer='he_uniform', padding=\"same\", activation='relu')(inp)\n",
    "    m = MaxPooling2D(2)(m)\n",
    "    # m = BatchNormalization()(m)\n",
    "    \n",
    "    m = Conv2D(64, (3,3), kernel_initializer='he_uniform', padding=\"same\", activation='relu')(m)\n",
    "    m = MaxPooling2D(2)(m)\n",
    "    # m = BatchNormalization()(m)\n",
    "    \n",
    "    m = Conv2D(128, (3,3), kernel_initializer='he_uniform', padding=\"same\", activation='relu')(m)\n",
    "    m = MaxPooling2D(2)(m)\n",
    "    \n",
    "#     m = Conv2D(128, (3,3), kernel_initializer='he_uniform', padding=\"same\", activation='relu')(m)\n",
    "#     m = MaxPooling2D(2)(m)\n",
    "    \n",
    "    \n",
    "#     m = Conv2D(256, (3,3), kernel_initializer='he_uniform', padding=\"same\", activation='relu')(m)\n",
    "#     m = MaxPooling2D(2)(m)\n",
    "    \n",
    "    m = Flatten()(m)\n",
    "    \n",
    "    m = Dense(128, activation = \"relu\")(m)\n",
    "    out = Dense(1, activation = \"sigmoid\")(m)\n",
    "    model = Model(inp, out)\n",
    "    model.compile(optimizer = keras.optimizers.SGD(1e-3, momentum=0.9), loss=\"binary_crossentropy\", metrics = ['acc'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 50, 50, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 50, 50, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 25, 25, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 25, 25, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 683,329\n",
      "Trainable params: 683,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = form_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "start_fit = now.strftime(\"%H:%M:%S.%f\")\n",
    "print(f'Time Before Model Fit: {start_fit}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 109404 samples, validate on 27352 samples\n",
      "Epoch 1/25\n",
      "109404/109404 [==============================] - 688s 6ms/sample - loss: 0.5336 - acc: 0.7643 - val_loss: 0.4351 - val_acc: 0.8232\n",
      "Epoch 2/25\n",
      "109404/109404 [==============================] - 810s 7ms/sample - loss: 0.4791 - acc: 0.7894 - val_loss: 0.4143 - val_acc: 0.8276\n",
      "Epoch 3/25\n",
      "109404/109404 [==============================] - 811s 7ms/sample - loss: 0.4657 - acc: 0.7966 - val_loss: 0.4285 - val_acc: 0.8203\n",
      "Epoch 4/25\n",
      "109404/109404 [==============================] - 669s 6ms/sample - loss: 0.4592 - acc: 0.7988 - val_loss: 0.4057 - val_acc: 0.8288\n",
      "Epoch 5/25\n",
      "109404/109404 [==============================] - 669s 6ms/sample - loss: 0.4548 - acc: 0.8008 - val_loss: 0.4040 - val_acc: 0.8285\n",
      "Epoch 6/25\n",
      "109404/109404 [==============================] - 707s 6ms/sample - loss: 0.4545 - acc: 0.8003 - val_loss: 0.4009 - val_acc: 0.8286\n",
      "Epoch 7/25\n",
      "109404/109404 [==============================] - 692s 6ms/sample - loss: 0.4496 - acc: 0.8031 - val_loss: 0.4017 - val_acc: 0.8287\n",
      "Epoch 8/25\n",
      "109404/109404 [==============================] - 680s 6ms/sample - loss: 0.4479 - acc: 0.8033 - val_loss: 0.4038 - val_acc: 0.8278\n",
      "Epoch 9/25\n",
      "109404/109404 [==============================] - 668s 6ms/sample - loss: 0.4417 - acc: 0.8060 - val_loss: 0.3967 - val_acc: 0.8306\n",
      "Epoch 10/25\n",
      "109404/109404 [==============================] - 761s 7ms/sample - loss: 0.4396 - acc: 0.8067 - val_loss: 0.4108 - val_acc: 0.8231\n",
      "Epoch 11/25\n",
      "109404/109404 [==============================] - 988s 9ms/sample - loss: 0.4392 - acc: 0.8063 - val_loss: 0.4218 - val_acc: 0.8168\n",
      "Epoch 12/25\n",
      "109404/109404 [==============================] - 988s 9ms/sample - loss: 0.4383 - acc: 0.8064 - val_loss: 0.4140 - val_acc: 0.8207\n",
      "Epoch 13/25\n",
      "109404/109404 [==============================] - 1060s 10ms/sample - loss: 0.4327 - acc: 0.8091 - val_loss: 0.4206 - val_acc: 0.8170\n",
      "Epoch 14/25\n",
      "109404/109404 [==============================] - 959s 9ms/sample - loss: 0.4328 - acc: 0.8094 - val_loss: 0.4127 - val_acc: 0.8210\n",
      "Epoch 15/25\n",
      "109404/109404 [==============================] - 796s 7ms/sample - loss: 0.4319 - acc: 0.8091 - val_loss: 0.4243 - val_acc: 0.8145\n",
      "Epoch 16/25\n",
      "109404/109404 [==============================] - 584s 5ms/sample - loss: 0.4271 - acc: 0.8109 - val_loss: 0.4063 - val_acc: 0.8259\n",
      "Epoch 17/25\n",
      "109404/109404 [==============================] - 565s 5ms/sample - loss: 0.4264 - acc: 0.8112 - val_loss: 0.4156 - val_acc: 0.8205\n",
      "Epoch 18/25\n",
      "109404/109404 [==============================] - 555s 5ms/sample - loss: 0.4261 - acc: 0.8114 - val_loss: 0.3919 - val_acc: 0.8317\n",
      "Epoch 19/25\n",
      "109404/109404 [==============================] - 563s 5ms/sample - loss: 0.4226 - acc: 0.8123 - val_loss: 0.4105 - val_acc: 0.8217\n",
      "Epoch 20/25\n",
      "109404/109404 [==============================] - 567s 5ms/sample - loss: 0.4196 - acc: 0.8142 - val_loss: 0.3910 - val_acc: 0.8319\n",
      "Epoch 21/25\n",
      "109404/109404 [==============================] - 568s 5ms/sample - loss: 0.4253 - acc: 0.8111 - val_loss: 0.3976 - val_acc: 0.8301\n",
      "Epoch 22/25\n",
      "109404/109404 [==============================] - 572s 5ms/sample - loss: 0.4194 - acc: 0.8138 - val_loss: 0.3969 - val_acc: 0.8307\n",
      "Epoch 23/25\n",
      "109404/109404 [==============================] - 572s 5ms/sample - loss: 0.4184 - acc: 0.8148 - val_loss: 0.3980 - val_acc: 0.8304\n",
      "Epoch 24/25\n",
      "109404/109404 [==============================] - 569s 5ms/sample - loss: 0.4147 - acc: 0.8169 - val_loss: 0.3871 - val_acc: 0.8330\n",
      "Epoch 25/25\n",
      "109404/109404 [==============================] - 577s 5ms/sample - loss: 0.4174 - acc: 0.8156 - val_loss: 0.3933 - val_acc: 0.8324\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs = 25, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "end_fit = now.strftime(\"%H:%M:%S.%f\")\n",
    "print(f'Time After Model Fit: {end_fit}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "start_eval = now.strftime(\"%H:%M:%S.%f\")\n",
    "print(f'Time Before Model Evaluate Test Data: {start_eval}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.3933380467260954\n",
      "Test accuracy: 0.8324071\n"
     ]
    }
   ],
   "source": [
    "# evaluating and printing results \n",
    "score = model.evaluate(X_test, y_test, verbose = 0) \n",
    "print('Test loss:', score[0]) \n",
    "print('Test accuracy:', score[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "end_eval = now.strftime(\"%H:%M:%S.%f\")\n",
    "print(f'Time After Model Evaluate Test Data: {end_eval}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "\n",
    "res = []\n",
    "for prediction in pred:\n",
    "    if(prediction > 0.5):\n",
    "        res.append(1)\n",
    "    else:\n",
    "        res.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10483  2871]\n",
      " [ 1713 12285]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.79      0.82     13354\n",
      "           1       0.81      0.88      0.84     13998\n",
      "\n",
      "    accuracy                           0.83     27352\n",
      "   macro avg       0.84      0.83      0.83     27352\n",
      "weighted avg       0.83      0.83      0.83     27352\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test, res))\n",
    "print(classification_report(y_test, res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
