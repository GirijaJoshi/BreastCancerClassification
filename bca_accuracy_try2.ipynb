{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Girija Joshi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Girija Joshi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Girija Joshi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Girija Joshi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Girija Joshi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Girija Joshi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Girija Joshi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Girija Joshi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Girija Joshi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Girija Joshi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Girija Joshi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Girija Joshi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait getting all images.....\n"
     ]
    }
   ],
   "source": [
    "# getting names of all the files in IDC_regular_ps50_idx5and sub dirs\n",
    "print(\"Please wait getting all images.....\")\n",
    "files = glob('Resources/IDC_regular_ps50_idx5/*/*/*')\n",
    "\n",
    "# Example : Resources/IDC_regular_ps50_idx5\\\\10254\\\\0\\\\10254_idx5_x1001_y1001_class0.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking how many are cancer files which has class1 in it\n",
    "count =0 \n",
    "for file in files:\n",
    "    if 'class1' in file:\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "Number Of 1: 129765\n",
      "Number Of 0: 123272\n",
      "Total: 253037\n"
     ]
    }
   ],
   "source": [
    "print('------------------')\n",
    "\n",
    "print(f'Number Of 1: {count}')\n",
    "print(f'Number Of 0: {len(files) - count}')\n",
    "# total number of files\n",
    "print(f'Total: {len(files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 80% of total for training and 20% for testing\n",
    "train_num = int(len(files) * 0.80)\n",
    "test_num = len(files) - train_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and y train data\n",
    "def find_data(files, lower_limit, upper_limit):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # tqdm(patient_ids)\n",
    "    for file in tqdm(files[lower_limit:upper_limit]):\n",
    "        if file.endswith(\".png\"):\n",
    "            # Convering cureent image into PIL image format. PIL image format is RGB format.\n",
    "            img = tf.keras.preprocessing.image.load_img(file, target_size = (50,50))\n",
    "\n",
    "            # Keras provides the img_to_array() function for converting a loaded image in PIL format into a NumPy array \n",
    "            #for use with deep learning models. The image is convertated into t [height, width, channels]\n",
    "\n",
    "            # # Arguments\n",
    "            #         img: PIL Image instance.\n",
    "            #         data_format: Image data format,\n",
    "            #             either \"channels_first\" or \"channels_last\".\n",
    "            #         dtype: Dtype to use for the returned array.\n",
    "            # Returns: A 3D Numpy array.\n",
    "            \n",
    "            # The component values are often stored as integer numbers in the range 0 to 255, \n",
    "            # the range that a single 8-bit byte can offer,\n",
    "            pixels = tf.keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "            # converting into 0 - 1, This is called as Normalization will help you to remove distortions \n",
    "            # caused by lights and shadows in an image.\n",
    "            pixels /= 255\n",
    "            X.append(pixels)\n",
    "            if 'class1' in file:\n",
    "                y.append(1)\n",
    "            elif 'class0' in file:\n",
    "                y.append(0)\n",
    "    return np.stack(X), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of training files: 202429\n",
      "Num of test files:50608\n"
     ]
    }
   ],
   "source": [
    "print(f'Num of training files: {train_num}\\nNum of test files:{test_num}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 202429/202429 [07:08<00:00, 472.21it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train = find_data(files,0, train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 50608/50608 [02:48<00:00, 300.99it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = find_data(files, train_num, len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='count'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='count'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Conv2D, Activation, MaxPooling2D, Flatten, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202429, 50, 50, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train2 is 4 diemtion we need to convert in 2D\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_model(inp_shape = (50,50,3)):\n",
    "    inp = Input(inp_shape)\n",
    "    m = Conv2D(32, (3,3), kernel_initializer='he_uniform', padding=\"same\", activation='relu')(inp)\n",
    "    m = MaxPooling2D(2)(m)\n",
    "    # m = BatchNormalization()(m)\n",
    "    \n",
    "    m = Conv2D(64, (3,3), kernel_initializer='he_uniform', padding=\"same\", activation='relu')(m)\n",
    "    m = MaxPooling2D(2)(m)\n",
    "    # m = BatchNormalization()(m)\n",
    "    \n",
    "    m = Conv2D(128, (3,3), kernel_initializer='he_uniform', padding=\"same\", activation='relu')(m)\n",
    "    m = MaxPooling2D(2)(m)\n",
    "    \n",
    "#     m = Conv2D(128, (3,3), kernel_initializer='he_uniform', padding=\"same\", activation='relu')(m)\n",
    "#     m = MaxPooling2D(2)(m)\n",
    "    \n",
    "    \n",
    "#     m = Conv2D(256, (3,3), kernel_initializer='he_uniform', padding=\"same\", activation='relu')(m)\n",
    "#     m = MaxPooling2D(2)(m)\n",
    "    \n",
    "    m = Flatten()(m)\n",
    "    \n",
    "    m = Dense(128, activation = \"relu\")(m)\n",
    "    out = Dense(1, activation = \"sigmoid\")(m)\n",
    "    model = Model(inp, out)\n",
    "    model.compile(optimizer = keras.optimizers.SGD(1e-3, momentum=0.9), loss=\"binary_crossentropy\", metrics = ['acc'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 50, 50, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 50, 50, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 25, 25, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 25, 25, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 683,329\n",
      "Trainable params: 683,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = form_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 202429 samples, validate on 50608 samples\n",
      "Epoch 1/25\n",
      " 25088/202429 [==>...........................] - ETA: 24:16 - loss: 0.5184 - acc: 0.7444"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-2237d3fd565f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    871\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 873\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    874\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3215\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3216\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3217\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3218\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n\u001b[0;32m   3219\u001b[0m                                  [x.numpy() for x in outputs])\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    556\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m    557\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m--> 558\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    625\u001b[0m     \u001b[1;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args)\u001b[0m\n\u001b[0;32m    413\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[0;32m    414\u001b[0m                    \"config_proto\", config),\n\u001b[1;32m--> 415\u001b[1;33m             ctx=ctx)\n\u001b[0m\u001b[0;32m    416\u001b[0m       \u001b[1;31m# Replace empty list with None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     59\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs = 25, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating and printing results \n",
    "score = model.evaluate(X_test, y_test, verbose = 0) \n",
    "print('Test loss:', score[0]) \n",
    "print('Test accuracy:', score[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "\n",
    "res = []\n",
    "for prediction in pred:\n",
    "    if(prediction > 0.5):\n",
    "        res.append(1)\n",
    "    else:\n",
    "        res.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test2, res))\n",
    "print(classification_report(y_test2, res))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
